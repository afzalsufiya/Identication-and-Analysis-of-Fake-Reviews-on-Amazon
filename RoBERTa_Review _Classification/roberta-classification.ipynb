{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa Pre-processing & Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for GPU\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining library for encoding the label\n",
    "encoded_label_dict = {\"CG\" : 0, \"OR\" : 1}\n",
    "def encode_label(x):\n",
    "    return encoded_label_dict.get(x,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dummy data\n",
    "df = pd.read_csv(\"TrainingDataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating column \"target\" for encoded label result\n",
    "df[\"target\"] = df[\"label\"].apply(lambda x: encode_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring roberta base model\n",
    "model_name = \"roberta-base\"\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afzal Sufiya\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initializing tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data for training by tokenizing text and creating input tensors\n",
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.text_[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.target[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (40432, 5)\n",
      "TRAIN Dataset: (32345, 5)\n",
      "VALID Dataset: (8087, 5)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader\n",
    "train_dataset, valid_dataset = train_test_split(df, test_size=0.2, shuffle=True, stratify=None, random_state=2021)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "valid_dataset = valid_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(valid_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **valid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the roberta model\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        tr_loss += loss\n",
    "        big_val, big_idx = torch.max(logits, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _!=0 and _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the validation function on the 20% of the dataset for testing the roberta model\n",
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            tr_loss += loss\n",
    "            big_val, big_idx = torch.max(logits, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _!=0 and _%100==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afzal Sufiya\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.6204878687858582\n",
      "Training Accuracy per 100 steps: 64.85148514851485\n",
      "Training Loss per 100 steps: 0.5184564590454102\n",
      "Training Accuracy per 100 steps: 72.76119402985074\n",
      "Training Loss per 100 steps: 0.41927778720855713\n",
      "Training Accuracy per 100 steps: 79.11129568106313\n",
      "Training Loss per 100 steps: 0.36729440093040466\n",
      "Training Accuracy per 100 steps: 82.07605985037407\n",
      "Training Loss per 100 steps: 0.3251354694366455\n",
      "Training Accuracy per 100 steps: 84.3063872255489\n",
      "Training Loss per 100 steps: 0.3007788062095642\n",
      "Training Accuracy per 100 steps: 85.64891846921797\n",
      "Training Loss per 100 steps: 0.2724084258079529\n",
      "Training Accuracy per 100 steps: 87.14336661911555\n",
      "Training Loss per 100 steps: 0.25458914041519165\n",
      "Training Accuracy per 100 steps: 88.04619225967541\n",
      "Training Loss per 100 steps: 0.2413390725851059\n",
      "Training Accuracy per 100 steps: 88.73473917869035\n",
      "Training Loss per 100 steps: 0.2317320704460144\n",
      "Training Accuracy per 100 steps: 89.31068931068931\n",
      "Training Loss per 100 steps: 0.2219620645046234\n",
      "Training Accuracy per 100 steps: 89.83878292461398\n",
      "Training Loss per 100 steps: 0.2129599153995514\n",
      "Training Accuracy per 100 steps: 90.33097418817653\n",
      "Training Loss per 100 steps: 0.20443043112754822\n",
      "Training Accuracy per 100 steps: 90.728285933897\n",
      "Training Loss per 100 steps: 0.19825638830661774\n",
      "Training Accuracy per 100 steps: 91.07780157030692\n",
      "Training Loss per 100 steps: 0.19199635088443756\n",
      "Training Accuracy per 100 steps: 91.42238507661558\n",
      "Training Loss per 100 steps: 0.1850740611553192\n",
      "Training Accuracy per 100 steps: 91.78638351030605\n",
      "Training Loss per 100 steps: 0.18066191673278809\n",
      "Training Accuracy per 100 steps: 92.06349206349206\n",
      "Training Loss per 100 steps: 0.17427051067352295\n",
      "Training Accuracy per 100 steps: 92.37923375902277\n",
      "Training Loss per 100 steps: 0.17100514471530914\n",
      "Training Accuracy per 100 steps: 92.56312467122567\n",
      "Training Loss per 100 steps: 0.16787967085838318\n",
      "Training Accuracy per 100 steps: 92.67241379310344\n",
      "Training Loss per 100 steps: 0.16360536217689514\n",
      "Training Accuracy per 100 steps: 92.86649214659685\n",
      "Training Loss per 100 steps: 0.15911565721035004\n",
      "Training Accuracy per 100 steps: 93.0542935029532\n",
      "Training Loss per 100 steps: 0.1565711349248886\n",
      "Training Accuracy per 100 steps: 93.20404172099087\n",
      "Training Loss per 100 steps: 0.15332771837711334\n",
      "Training Accuracy per 100 steps: 93.34652228238234\n",
      "Training Loss per 100 steps: 0.15155760943889618\n",
      "Training Accuracy per 100 steps: 93.43762495002\n",
      "Training Loss per 100 steps: 0.14866144955158234\n",
      "Training Accuracy per 100 steps: 93.57458669742407\n",
      "Training Loss per 100 steps: 0.14632046222686768\n",
      "Training Accuracy per 100 steps: 93.68289522399111\n",
      "Training Loss per 100 steps: 0.14395610988140106\n",
      "Training Accuracy per 100 steps: 93.79239557300964\n",
      "Training Loss per 100 steps: 0.14238202571868896\n",
      "Training Accuracy per 100 steps: 93.85987590486039\n",
      "Training Loss per 100 steps: 0.14067940413951874\n",
      "Training Accuracy per 100 steps: 93.94785071642785\n",
      "Training Loss per 100 steps: 0.13827146589756012\n",
      "Training Accuracy per 100 steps: 94.0744920993228\n",
      "Training Loss per 100 steps: 0.1357513964176178\n",
      "Training Accuracy per 100 steps: 94.16588566073102\n",
      "Training Loss per 100 steps: 0.13370506465435028\n",
      "Training Accuracy per 100 steps: 94.25174189639503\n",
      "Training Loss per 100 steps: 0.13163499534130096\n",
      "Training Accuracy per 100 steps: 94.350926198177\n",
      "Training Loss per 100 steps: 0.1305292397737503\n",
      "Training Accuracy per 100 steps: 94.40159954298771\n",
      "Training Loss per 100 steps: 0.12925352156162262\n",
      "Training Accuracy per 100 steps: 94.47375728964177\n",
      "Training Loss per 100 steps: 0.12746582925319672\n",
      "Training Accuracy per 100 steps: 94.56228046473926\n",
      "Training Loss per 100 steps: 0.12707747519016266\n",
      "Training Accuracy per 100 steps: 94.60339384372533\n",
      "Training Loss per 100 steps: 0.12557043135166168\n",
      "Training Accuracy per 100 steps: 94.68405537041784\n",
      "Training Loss per 100 steps: 0.12403354048728943\n",
      "Training Accuracy per 100 steps: 94.74193951512122\n",
      "The Total Accuracy for Epoch 0: 94.77198948832896\n",
      "Training Loss Epoch: 0.12342124432325363\n",
      "Training Accuracy Epoch: 94.77198948832896\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 0.10914777219295502\n",
      "Validation Accuracy per 100 steps: 96.41089108910892\n",
      "Validation Loss per 100 steps: 0.1075654849410057\n",
      "Validation Accuracy per 100 steps: 96.20646766169155\n",
      "Validation Loss per 100 steps: 0.09874396026134491\n",
      "Validation Accuracy per 100 steps: 96.30398671096346\n",
      "Validation Loss per 100 steps: 0.1022157371044159\n",
      "Validation Accuracy per 100 steps: 96.25935162094763\n",
      "Validation Loss per 100 steps: 0.09976135194301605\n",
      "Validation Accuracy per 100 steps: 96.20758483033931\n",
      "Validation Loss per 100 steps: 0.09803064167499542\n",
      "Validation Accuracy per 100 steps: 96.21464226289517\n",
      "Validation Loss per 100 steps: 0.09710319340229034\n",
      "Validation Accuracy per 100 steps: 96.27318116975749\n",
      "Validation Loss per 100 steps: 0.09768930822610855\n",
      "Validation Accuracy per 100 steps: 96.28589263420724\n",
      "Validation Loss per 100 steps: 0.10152488946914673\n",
      "Validation Accuracy per 100 steps: 96.1431742508324\n",
      "Validation Loss per 100 steps: 0.10288810729980469\n",
      "Validation Accuracy per 100 steps: 96.16633366633367\n",
      "Validation Loss Epoch: 0.10226370394229889\n",
      "Validation Accuracy Epoch: 96.19141832570793\n",
      "Accuracy on validation data = 96.19%\n"
     ]
    }
   ],
   "source": [
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on validation data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save the model weights\n",
    "output_model_file = 'C:/Users/Afzal Sufiya/Documents/MasterThesis/Review-Classification/FakeRoberta/roberta-pretrained1.pt'\n",
    "\n",
    "model_to_save=model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "\n",
    "print('All files saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afzal Sufiya\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading saved weights\n",
    "model = torch.load('C:/Users/Afzal Sufiya/Documents/MasterThesis/Review-Classification/FakeRoberta/roberta-pretrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Probability: 0.9975130558013916\n",
      "Fake Probability: 0.0024869043845683336\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "query = \"\"\"I work in the wedding industry and have to work long days, on my feet, outside in the heat, and have to look professional. I've spent a ridiculous amount of money on high end dress shoes like Merrels and just have not been able to find a pair that are comfortable to wear all day. Both for my feet and my back. Enter the Sanuk yoga sling!!! These shoes are amazingly comfortable. Though, I will admit it took a few wears to get used to the feel of the yoga matte bottom. At first, it felt a little \"sticky\" to me, and the fabric part that goes through the toe area was a little thick and took some getting used to. I wore them for a few days before taking them out on a job and I can't get over how comfortable they are. Ii have been wearing these shoes now for 3 months, every work day and I am THRILLED. No more back pain, no more sore feet. I also wear these sometimes during my off time,mans every time I wear them, I get compliments on how cute and comfortable they look. The great thing about these shoes is the yoga matte bottom. It helps your feet grip to the shoe a bit, so your foot can just walk normally, without having to grip the shoe. You may not realize it, but with a lot of Sandals, your foot is having to work to keep the shoe on, changing the way you walk and stand and ultimately causing foot and back pain. Not with these! Also, the soft linen sits comfortably on your skin and breathes nicely in the heat. The only downside is the funky tan lines, which is why I am sure to alternate shoes on my days off, especially if I plan to be outside for most of the day. If it were not for that, I think these might be the only shoes I'd wear all summer. If you are looking for a reasonable priced, comfortable shoe that you can wear and walk in all day.\"\"\"\n",
    "tokens = tokenizer.encode(query,return_tensors=\"pt\")\n",
    "all_tokens = len(tokens)\n",
    "mask = torch.ones_like(tokens)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "    probs = logits.softmax(dim=-1)\n",
    "\n",
    "fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "\n",
    "print(f\"Real Probability: {real}\\nFake Probability: {fake}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Probability: 0.0002426303835818544\n",
      "Fake Probability: 0.9997573494911194\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "query = \"\"\"My old bet was wearing this to the Macy's in January.  This is the first one I've ever had.  I am a 32D, and the first pair I bought were just a little tight.  I'm a bit disappointed.  This is my second pair.  I'm looking forward to wearing them to the Macy's in the fall.  I like the way they look.Love these!These are my favorite.  I have a hard time finding jeans that fit me comfortably, but I have a hard time finding jeans that don't fit.  These jeans are super comfortable and have a great price point.  I have some great jeans to wear for work, but these are the only jeans that I wear for work or for my family.  I will be buying more!  I have a lot of compliments on them.I love these shoes. I love the color and the fit. They fit my body well and are comfortable. I have a wide foot and these fit me well.\n",
    "\n",
    "I'm 5'4\", 130lbs and these fit well. I would recommend them.I wear a size 11.5 in jeans and this fits perfect. I have a narrow foot and this fits perfect. It is very comfortable and fits great. I bought a small and it fit perfectly. I will order another size up.I bought these for my husband, he loves them and he loves them!This is the best pair of sunglasses for the price!  They are so comfortable and easy to use.  I wear them all the time and they don't hurt my feet.  I wear them everyday and my feet are so happy with them!\"\"\"\n",
    "tokens = tokenizer.encode(query,return_tensors=\"pt\")\n",
    "all_tokens = len(tokens[0])\n",
    "mask = torch.ones_like(tokens)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "    probs = logits.softmax(dim=-1)\n",
    "\n",
    "fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "\n",
    "print(f\"Real Probability: {real}\\nFake Probability: {fake}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, model, tokenizer, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975046515464783"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"I work in the wedding industry and have to work long days, on my feet, outside in the heat, and have to look professional. I've spent a ridiculous amount of money on high end dress shoes like Merrels and just have not been able to find a pair that are comfortable to wear all day. Both for my feet and my back. Enter the Sanuk yoga sling!!! These shoes are amazingly comfortable. Though, I will admit it took a few wears to get used to the feel of the yoga matte bottom. At first, it felt a little \"sticky\" to me, and the fabric part that goes through the toe area was a little thick and took some getting used to. I wore them for a few days before taking them out on a job and I can't get over how comfortable they are. Ii have been wearing these shoes now for 3 months, every work day and I am THRILLED. No more back pain, no more sore feet. I also wear these sometimes during my off time,mans every time I wear them, I get compliments on how cute and comfortable they look. The great thing about these shoes is the yoga matte bottom. It helps your feet grip to the shoe a bit, so your foot can just walk normally, without having to grip the shoe. You may not realize it, but with a lot of Sandals, your foot is having to work to keep the shoe on, changing the way you walk and stand and ultimately causing foot and back pain. Not with these! Also, the soft linen sits comfortably on your skin and breathes nicely in the heat. The only downside is the funky tan lines, which is why I am sure to alternate shoes on my days off, especially if I plan to be outside for most of the day. If it were not for that, I think these might be the only shoes I'd wear all summer. If you are looking for a reasonable priced, comfortable shoe that you can wear and walk in all day.\"\"\"\n",
    "predict(query,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m preds, preds_probas \u001b[38;5;241m=\u001b[39m [],[]\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalid_dataset\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      3\u001b[0m     query \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m     pred \u001b[38;5;241m=\u001b[39m predict(query,model,tokenizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "preds, preds_probas = [],[]\n",
    "for i, row in valid_dataset.iterrows():\n",
    "    query = row[\"text_\"]\n",
    "    pred = predict(query,model,tokenizer)\n",
    "    preds_probas.append(pred)\n",
    "    if pred >= 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3935,   75],\n",
       "       [ 181, 3896]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = valid_dataset.target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred)\n",
    "recall = recall_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83442562136763; Precision:98.1113069755729; Recall:95.56046112337503\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m(y_true, y_pred, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCG\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOR\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"CG\",\"OR\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on Actual Dataset with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model weights\n",
    "model_path = 'C:/Users/Afzal Sufiya/Documents/MasterThesis/Review-Classification/FakeRoberta/roberta-pretrained1.pt'\n",
    "model = torch.load(model_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading data from database\n",
    "def load_data_from_db(db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "# Function for saving data back to database\n",
    "def save_predictions_to_db(db_path, table_name, df, result_column=\"result\"):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Add a new column if it doesn't exist\n",
    "    cursor.execute(f\"ALTER TABLE {table_name} ADD COLUMN {result_column} REAL\")\n",
    "    \n",
    "    # Update each row with the prediction\n",
    "    for index, row in df.iterrows():\n",
    "        cursor.execute(f\"UPDATE {table_name} SET {result_column} = ? WHERE id = ?\", (row[result_column], index + 1))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Loading the database\n",
    "db_path = \"C:/Users/Afzal Sufiya/Documents/MasterThesis/AmazonReviews-predicted.db\"  \n",
    "table_name = \"reviews\"  \n",
    "new_df = load_data_from_db(db_path, table_name)\n",
    "\n",
    "# column name used for prediction\n",
    "text_column = \"description\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# class for the dataset\n",
    "class NewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, text_column):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.text_column = text_column\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data[self.text_column][index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataLoader for the actual dataset\n",
    "new_dataset = NewDataset(new_df, tokenizer, MAX_LEN, text_column)\n",
    "new_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "new_loader = DataLoader(new_dataset, **new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            outputs = model(ids, attention_mask=mask)\n",
    "            logits = outputs.logits\n",
    "            probs = logits.softmax(dim=-1)\n",
    "            preds = probs[:, 1].cpu().numpy() \n",
    "            predictions.extend(preds)\n",
    "    return predictions\n",
    "\n",
    "# predicting for the actual dataset\n",
    "predictions = predict(model, new_loader, device)\n",
    "\n",
    "# Adding predictions to the dataframe\n",
    "new_df[\"result\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the predictions back to the database\n",
    "save_predictions_to_db(db_path, table_name, new_df)\n",
    "\n",
    "print(\"Predictions saved to the database\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
